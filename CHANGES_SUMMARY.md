# ì½”ë“œ ë³€ê²½ ì‚¬í•­ ìš”ì•½

## ê°œìš”

[finetune_consistency.py](src/alpamayo_r1/alignment/finetune_consistency.py)ë¥¼ **Self-Reflective Denoising RL (SRD-RL)** ë°©ì‹ìœ¼ë¡œ ëŒ€í­ ê°œì„ í–ˆìŠµë‹ˆë‹¤.

---

## ì£¼ìš” ë³€ê²½ ì‚¬í•­

### 1. ìƒˆë¡œìš´ í•¨ìˆ˜ ì¶”ê°€ (ë¼ì¸ 108-331)

#### Visual Safety Scoring Functions
- `compute_texture_variance()`: Laplacian varianceë¡œ ì§€í˜• ê±°ì¹ ê¸° ì¸¡ì •
- `compute_color_consistency()`: ê²½ë¡œ ìƒ‰ìƒê³¼ ì•ˆì „ ê¸°ì¤€ ì˜ì—­ ë¹„êµ
- `compute_visual_safety_score()`: ì¢…í•© ì•ˆì „ì„± ì ìˆ˜ ê³„ì‚°

#### Reasoning Analysis Functions
- `detect_danger_keywords()`: "mud", "rock", "obstacle" ë“± ìœ„í—˜ í‚¤ì›Œë“œ íƒì§€
- `check_reasoning_trajectory_alignment()`: ì–¸ì–´ì™€ í–‰ë™ì˜ ë…¼ë¦¬ì  ì¼ì¹˜ ê²€ì¦

### 2. ConsistencyEnhancedModel í´ë˜ìŠ¤ ê°œì„ 

#### ìƒˆë¡œìš´ ì´ˆê¸°í™” íŒŒë¼ë¯¸í„° (ë¼ì¸ 544-568)
```python
safety_reward_weight: float = 1.5
gt_reward_weight: float = 0.5
reasoning_reward_weight: float = 0.3
num_trajectory_samples: int = 4
rl_loss_weight: float = 0.5
danger_threshold: float = 0.3
gt_trust_min: float = 0.1
gt_trust_max: float = 1.0
```

#### ìƒˆë¡œìš´ forward ë©”ì„œë“œ ë¡œì§ (ë¼ì¸ 661-727)
- `reasoning_text` íŒŒë¼ë¯¸í„° ì¶”ê°€
- RL loss ê³„ì‚° ì¶”ê°€ (2b ì„¹ì…˜)

#### **í•µì‹¬: _compute_rl_loss() ë©”ì„œë“œ ì¶”ê°€ (ë¼ì¸ 903-1192)**

**AWR ìŠ¤íƒ€ì¼ RL êµ¬í˜„ (ê°œì„ !):**

```python
def _compute_rl_loss(self, ...):
    # 1. âœ… ì‹¤ì œ ëª¨ë¸ì—ì„œ trajectory ìƒ˜í”Œë§ (NOT random noise!)
    for i in range(num_trajectory_samples):
        # Diffusionì—ì„œ ë‹¤ì–‘í•œ noise seedë¡œ ìƒ˜í”Œë§
        sampled_action = diffusion.sample(
            expert,
            init_noise=torch.randn(...) * (1.0 + i * 0.2)
        )
        sampled_traj = action_to_traj(sampled_action)

    # 2. ê° ìƒ˜í”Œì˜ ë³´ìƒ ê³„ì‚°
    for traj in sampled_trajectories:
        safety_score = compute_visual_safety_score(image, traj)
        gt_similarity = exp(-distance(traj, gt))
        reasoning_alignment = check_alignment(reasoning, traj)

        # **Trust Gate**: Dynamic GT weighting
        if detect_danger(reasoning) and safety_score < 0.3:
            gt_weight = 0.1  # DISTRUST GT!
        else:
            gt_weight = 1.0  # Trust GT

        reward = safety_weight * safety_score + \
                 gt_weight * gt_similarity + \
                 reasoning_weight * reasoning_alignment

    # 3. âœ… AWR: ìµœê³  ë³´ìƒ trajectoryë¡œ í•™ìŠµ (ë” ì•ˆì •ì !)
    best_idx = reward.argmax()
    best_traj = sampled_trajectories[best_idx]
    weight = exp(advantage[best_idx] / temperature)

    # Flow matching loss to best trajectory (with gradient!)
    rl_loss = weighted_mse(model(best_traj), best_traj, weight)

    return rl_loss
```

**ì£¼ìš” ê°œì„ ì **:
1. **ì‹¤ì œ ëª¨ë¸ ìƒ˜í”Œë§**: `gt + noise` â†’ `diffusion.sample()`
2. **AWR ì•Œê³ ë¦¬ì¦˜**: REINFORCE â†’ Advantage Weighted Regression (ë” ì•ˆì •ì )
3. **Gradient íë¦„**: no_grad ìƒ˜í”Œë§ + ë³„ë„ forwardë¡œ ì˜¬ë°”ë¥¸ gradient
4. **ë©”íŠ¸ë¦­ ì¶”ê°€**: `gt_is_best` (GTê°€ ìµœì„ ì¸ ë¹„ìœ¨)

### 3. ë°ì´í„° íŒŒì´í”„ë¼ì¸ ìˆ˜ì •

#### Dataset.__getitem__() (ë¼ì¸ 1175-1186)
- `reasoning_text` í•„ë“œ ì¶”ê°€í•˜ì—¬ ë°˜í™˜

#### collate_fn() (ë¼ì¸ 1229-1244)
- `reasoning_text` ë¦¬ìŠ¤íŠ¸ ì²˜ë¦¬ ì¶”ê°€

### 4. Training Loop ìˆ˜ì •

#### ëª¨ë¸ ì´ˆê¸°í™” (ë¼ì¸ 1350-1365)
- ëª¨ë“  SRD-RL í•˜ì´í¼íŒŒë¼ë¯¸í„° ì „ë‹¬

#### Forward í˜¸ì¶œ (ë¼ì¸ 1446-1456)
- `reasoning_text=batch.get("reasoning_text")` ì¶”ê°€

#### ë¡œê¹… ê°œì„  (ë¼ì¸ 1467-1481)
- RL ë©”íŠ¸ë¦­ í‘œì‹œ ìµœì í™”

### 5. Training Arguments í™•ì¥ (ë¼ì¸ 73-105)

ìƒˆë¡œìš´ íŒŒë¼ë¯¸í„°:
```python
safety_reward_weight: float = 1.5
gt_reward_weight: float = 0.5
reasoning_reward_weight: float = 0.3
num_trajectory_samples: int = 4
rl_loss_weight: float = 0.5
danger_keyword_threshold: float = 0.3
gt_trust_min: float = 0.1
gt_trust_max: float = 1.0
```

---

## ì½”ë“œ í¬ê¸° ë³€í™”

- **ì´ì „**: ~1,082 ë¼ì¸
- **ì´í›„**: ~1,550 ë¼ì¸
- **ì¶”ê°€**: ~470 ë¼ì¸ (ì£¼ë¡œ RL ë¡œì§)

---

## ì˜ì¡´ì„± ì¶”ê°€

- `import cv2`: OpenCV (í…ìŠ¤ì²˜ ë¶„ì„ìš©)
- `import re`: ì •ê·œí‘œí˜„ì‹ (í‚¤ì›Œë“œ ê°ì§€ìš©)

---

## í•˜ìœ„ í˜¸í™˜ì„±

âœ… **ì™„ì „ í˜¸í™˜**: ê¸°ì¡´ í•™ìŠµ ëª…ë ¹ì–´ëŠ” ê·¸ëŒ€ë¡œ ì‘ë™í•©ë‹ˆë‹¤.

ìƒˆë¡œìš´ RL ê¸°ëŠ¥ì„ ë„ë ¤ë©´:
```bash
--rl_loss_weight 0.0
```

---

## í…ŒìŠ¤íŠ¸ ìƒíƒœ

âœ… Python ë¬¸ë²• ê²€ì¦ ì™„ë£Œ
â³ ì‹¤ì œ í•™ìŠµ í…ŒìŠ¤íŠ¸ í•„ìš”

---

## íŒŒì¼ êµ¬ì¡°

```
/home/byounggun/alpamayo/
â”œâ”€â”€ src/alpamayo_r1/alignment/
â”‚   â””â”€â”€ finetune_consistency.py      # â­ ë©”ì¸ ìˆ˜ì • íŒŒì¼
â”œâ”€â”€ SRD_RL_README.md                 # ğŸ“˜ ìƒì„¸ ë¬¸ì„œ
â”œâ”€â”€ QUICKSTART_SRD_RL.md             # ğŸš€ ë¹ ë¥¸ ì‹œì‘
â””â”€â”€ CHANGES_SUMMARY.md               # ğŸ“ ì´ íŒŒì¼
```

---

## ë‹¤ìŒ í•  ì¼

1. **ì‹¤ì œ ë°ì´í„°ë¡œ í•™ìŠµ í…ŒìŠ¤íŠ¸**
   ```bash
   cd /home/byounggun/alpamayo/src
   torchrun --nproc_per_node=2 -m alpamayo_r1.alignment.finetune_consistency \
       --data_path /path/to/finetune_data.jsonl \
       --output_dir /path/to/output \
       --per_device_train_batch_size 1 \
       --gradient_accumulation_steps 4 \
       --num_train_epochs 3 \
       --learning_rate 5e-6
   ```

2. **ë©”íŠ¸ë¦­ ëª¨ë‹ˆí„°ë§**
   - `rl_loss` ì•ˆì •ì„±
   - `safety` vs `gt_sim` íŠ¸ë ˆì´ë“œì˜¤í”„
   - `reward` ì¦ê°€ ì¶”ì„¸

3. **í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹**
   - ë°ì´í„° í’ˆì§ˆì— ë”°ë¼ `safety_reward_weight` / `gt_reward_weight` ì¡°ì •

4. **í‰ê°€**
   - í•™ìŠµëœ ëª¨ë¸ë¡œ ì¶”ë¡ 
   - GTë¥¼ ë¬´ì‹œí•œ ì¼€ì´ìŠ¤ ì‹œê°í™”
   - ì•ˆì „ì„± ê°œì„  ì¸¡ì •

---

## ê¸°ìˆ ì  í•˜ì´ë¼ì´íŠ¸

### ğŸ”¥ Trust Gate ì•Œê³ ë¦¬ì¦˜

```python
# ëª¨ë¸ì´ ìŠ¤ìŠ¤ë¡œ GTì˜ ì‹ ë¢°ë„ë¥¼ íŒë‹¨
if has_danger_keyword(reasoning) and visual_safety < threshold:
    # "ë‚´ê°€ ë³´ê¸°ì—” ìœ„í—˜í•œë° GTëŠ” ì§ì§„í•˜ë˜? GTê°€ í‹€ë ¸ì–´!"
    gt_weight = 0.1  # GT ê±°ì˜ ë¬´ì‹œ
else:
    # "í‰ë²”í•œ ìƒí™©, GT ë¯¿ì–´ë„ ë¼"
    gt_weight = 1.0  # GT ì‹ ë¢°
```

### ğŸ§  Visual Safety (No Depth!)

```python
# RGBë§Œìœ¼ë¡œ ìœ„í—˜ ì§€í˜• ê°ì§€
texture_safety = 1.0 - laplacian_variance(path)  # ê±°ì¹œ ë•… = ë†’ì€ ë¶„ì‚°
color_safety = consistency(path_color, safe_reference)
safety_score = 0.5 * texture_safety + 0.5 * color_safety
```

### ğŸ¯ GRPO-style Learning

```python
# ì—¬ëŸ¬ trajectory ìƒ˜í”Œë§ â†’ ë³´ìƒ ë¹„êµ â†’ ì¢‹ì€ ê²ƒ ê°•í™”
advantages = rewards - baseline
loss = -sum(log_prob[i] * advantage[i])  # Policy gradient
```

---

## ì—°êµ¬ ê¸°ì—¬

ì´ ì½”ë“œëŠ” ë‹¤ìŒ ë¶„ì•¼ì— ê¸°ì—¬í•©ë‹ˆë‹¤:

1. **Learning from Noisy Labels**: GT ì‹ ë¢°ë„ ë™ì  ì¡°ì ˆ
2. **Vision-Language-Action Alignment**: ë‹¤ì¤‘ ëª¨ë‹¬ ì¼ê´€ì„±
3. **Self-Supervised Denoising**: ëª¨ë¸ì´ ìŠ¤ìŠ¤ë¡œ ë…¸ì´ì¦ˆ ê°ì§€
4. **Safe RL for Autonomous Driving**: ì•ˆì „ì„± ê¸°ë°˜ ë³´ìƒ ì„¤ê³„

---

**ì§ˆë¬¸ì´ë‚˜ ë²„ê·¸ ë°œê²¬ ì‹œ ì´ìŠˆ ë¦¬í¬íŠ¸ ë¶€íƒë“œë¦½ë‹ˆë‹¤!**

# ğŸ¯ SRD-RL ìµœì¢… êµ¬í˜„ ìš”ì•½

## í•µì‹¬ ì§ˆë¬¸ê³¼ í•´ê²°ì±…

### â“ "ë™ì¼ ì´ë¯¸ì§€ì— ëŒ€í•´ ë‹¤ë¥¸ trajectoryë¥¼ ì–´ë–»ê²Œ ìƒì„±í•˜ë‚˜?"

**ë¬¸ì œ**: ì´ˆê¸° êµ¬í˜„ì—ì„œëŠ” `gt_trajectory + random_noise`ë¡œ ìƒ˜í”Œë§í–ˆëŠ”ë°, ì´ê±´:
- ëª¨ë¸ì˜ ì‹¤ì œ ì¶œë ¥ì´ ì•„ë‹˜
- Gradientê°€ ì œëŒ€ë¡œ íë¥´ì§€ ì•ŠìŒ
- ë‹¤ì–‘ì„± ë¶€ì¡±

**âœ… í•´ê²°ì±…**: **ì‹¤ì œ Diffusion ëª¨ë¸ì—ì„œ Stochastic Sampling**

```python
# âŒ ê¸°ì¡´ (ì˜ëª»ëœ ë°©ì‹)
for _ in range(N):
    noisy_traj = gt_trajectory + torch.randn_like(gt_trajectory) * 0.5

# âœ… ê°œì„  (ì˜¬ë°”ë¥¸ ë°©ì‹)
for i in range(N):
    # Diffusionì˜ ë‹¤ì–‘í•œ noise seedë¡œ ì‹¤ì œ ìƒ˜í”Œë§
    sampled_action = self.diffusion.sample(
        self.expert,
        batch_size=B,
        action_dim=action_dims,
        device=device,
        dtype=dtype,
        init_noise=torch.randn(...) * (1.0 + i * 0.2)  # ë‹¤ì–‘í•œ ì´ˆê¸°ê°’
    )

    # Actionì„ trajectoryë¡œ ë³€í™˜
    sampled_traj = self.action_space.action_to_traj(
        sampled_action,
        traj_history_xyz=ego_history_xyz,
        traj_history_rot=ego_history_rot,
    )['traj_future_xyz']
```

**ì™œ ì´ê²Œ ì‘ë™í•˜ëŠ”ê°€?**

1. **Diffusionì˜ Stochasticity**: Flow matching/diffusionì€ `x_0 ~ N(0, 1)`ì—ì„œ ì‹œì‘
2. **ë‹¤ë¥¸ ì´ˆê¸° ë…¸ì´ì¦ˆ** â†’ **ë‹¤ë¥¸ ìµœì¢… trajectory**
3. **ëª¨ë¸ì˜ ì‹¤ì œ ë¶„í¬**ì—ì„œ ìƒ˜í”Œë§ë¨

---

## ì•Œê³ ë¦¬ì¦˜ íë¦„ë„

```
Input: Image, GT trajectory, Ego history
â”‚
â”œâ”€> 1. Diffusionì—ì„œ Nê°œ trajectory ìƒ˜í”Œë§
â”‚    â”œâ”€ Sample 1: GT (baseline)
â”‚    â”œâ”€ Sample 2: diffusion.sample(noise_seed=1)
â”‚    â”œâ”€ Sample 3: diffusion.sample(noise_seed=2)
â”‚    â””â”€ Sample N: diffusion.sample(noise_seed=N)
â”‚
â”œâ”€> 2. ê° ìƒ˜í”Œì˜ ë³´ìƒ ê³„ì‚°
â”‚    â”‚
â”‚    â”œâ”€ Visual Safety Score
â”‚    â”‚   â”œâ”€ Laplacian variance (texture)
â”‚    â”‚   â””â”€ Color consistency
â”‚    â”‚
â”‚    â”œâ”€ GT Similarity Score
â”‚    â”‚   â””â”€ exp(-distance(sampled, GT))
â”‚    â”‚
â”‚    â”œâ”€ Reasoning Alignment Score
â”‚    â”‚   â””â”€ Language-action consistency
â”‚    â”‚
â”‚    â””â”€ **Trust Gate** (í•µì‹¬!)
â”‚        â”‚
â”‚        â”œâ”€ IF (reasoning has "danger") AND (safety < 0.3):
â”‚        â”‚   â””â”€ gt_weight = 0.1  â† GT ë¬´ì‹œ!
â”‚        â”‚
â”‚        â””â”€ ELSE:
â”‚            â””â”€ gt_weight = 1.0  â† GT ì‹ ë¢°
â”‚
â”‚    Final Reward = safety_weight * safety +
â”‚                    gt_weight * gt_similarity +
â”‚                    reasoning_weight * alignment
â”‚
â”œâ”€> 3. ìµœê³  ë³´ìƒ trajectory ì„ íƒ
â”‚    â””â”€ best_idx = argmax(rewards)
â”‚
â””â”€> 4. AWR Loss ê³„ì‚° (Gradient íë¦„!)
     â”‚
     â”œâ”€ Advantage weight: w = exp(advantage / temp)
     â”‚
     â””â”€ Flow matching loss:
         Forward model(best_trajectory) â†’ Weighted MSE
```

---

## í•µì‹¬ ê°œì„  ì‚¬í•­

### 1. ì‹¤ì œ ëª¨ë¸ ìƒ˜í”Œë§

**Before**:
```python
sampled = gt + noise  # ëª¨ë¸ ì•„ë‹˜!
```

**After**:
```python
sampled = diffusion.sample(expert, init_noise=...)  # ì‹¤ì œ ëª¨ë¸!
```

### 2. AWR vs REINFORCE

**REINFORCE (ë¶ˆì•ˆì •)**:
```python
loss = -log_prob * advantage  # High variance!
```

**AWR (ì•ˆì •ì )**:
```python
best_traj = trajectories[argmax(reward)]
weight = exp(advantage / temperature)
loss = weighted_mse(model(best_traj), best_traj, weight)  # Low variance!
```

### 3. Gradient íë¦„

**Before**:
```python
with torch.no_grad():
    samples = [gt + noise for _ in range(N)]
# âŒ No gradient!
```

**After**:
```python
# Sampling (no grad)
with torch.no_grad():
    samples = [diffusion.sample(...) for _ in range(N)]

# Training (with grad!)
target = samples[best_idx]
loss = flow_matching_loss(model(target), target)  # âœ… Gradient flows!
```

---

## ì£¼ìš” ë©”íŠ¸ë¦­ í•´ì„

í•™ìŠµ ì¤‘ ë‚˜íƒ€ë‚˜ëŠ” ë©”íŠ¸ë¦­:

```
rl_reward_mean: 2.456     # í‰ê·  ë³´ìƒ
rl_reward_best: 2.789     # ìµœê³  ë³´ìƒ
rl_gt_is_best: 0.65       # â­ í•µì‹¬ ë©”íŠ¸ë¦­!
rl_safety_mean: 0.678     # ì‹œê°ì  ì•ˆì „ì„±
rl_gt_sim_mean: 0.823     # GT ìœ ì‚¬ë„
rl_weight_mean: 1.23      # AWR ê°€ì¤‘ì¹˜
```

### ğŸ¯ `rl_gt_is_best`: ì„±ê³µì˜ ì§€í‘œ

- **1.0 (100%)**: í•­ìƒ GTê°€ ìµœì„  â†’ ëª¨ë¸ì´ ê°œì„  ëª»í•¨
- **0.65 (65%)**: 35%ëŠ” ëª¨ë¸ì´ GTë³´ë‹¤ ë‚˜ì€ ê²½ë¡œ ë°œê²¬!
- **0.30 (30%)**: 70%ëŠ” ëª¨ë¸ì´ GT ëŠ¥ê°€ â†’ ğŸ‰ ëª©í‘œ ë‹¬ì„±!

**ì´ìƒì  í•™ìŠµ ê³¡ì„ **:
```
Epoch 1:  gt_is_best = 0.90  (ëª¨ë¸ì´ ì•„ì§ ì•½í•¨)
Epoch 3:  gt_is_best = 0.70  (ê°œì„  ì¤‘)
Epoch 5:  gt_is_best = 0.50  (ì ˆë°˜ì€ ëª¨ë¸ì´ ë‚˜ìŒ!)
Epoch 10: gt_is_best = 0.30  (ëª©í‘œ ë‹¬ì„±!)
```

---

## ì‹¤ì „ ì˜ˆì œ

### ì‹œë‚˜ë¦¬ì˜¤: ì§„í™íƒ• ì§ì§„ GT

```python
# GT: "ì§„í™íƒ•ìœ¼ë¡œ ì§ì§„" (ì˜ëª»ëœ ë°ì´í„°!)
gt_trajectory = [[1, 0], [2, 0], [3, 0], ...]  # ì§ì§„

# 1. ëª¨ë¸ì´ 4ê°œ ìƒ˜í”Œ ìƒì„±
samples = [
    gt_trajectory,           # Sample 0: GT (ì§ì§„)
    [[1, 0.5], [2, 1.0], ...], # Sample 1: ì•½ê°„ ìš°íšŒ
    [[1, 1.5], [2, 3.0], ...], # Sample 2: í¬ê²Œ ìš°íšŒ
    [[1, -0.3], [2, -0.5], ...], # Sample 3: ì•½ê°„ ì¢ŒíšŒì „
]

# 2. ë³´ìƒ ê³„ì‚°
rewards = [
    1.2,  # GT: safety=0.2 (ì§„í™), gt_sim=1.0 â†’ gt_weight=0.1 ì ìš©!
    2.8,  # Sample 1: safety=0.9 (ì•ˆì „), gt_sim=0.7
    3.1,  # Sample 2: safety=1.0 (ë§¤ìš° ì•ˆì „), gt_sim=0.4 â† BEST!
    2.3,  # Sample 3: safety=0.8, gt_sim=0.8
]

# 3. Trust Gate ì‘ë™
# Reasoning: "I see mud ahead"
# Safety: 0.2 < 0.3 (danger!)
# â†’ gt_weight = 0.1 (GT ë¬´ì‹œ!)

# 4. AWR í•™ìŠµ
best_idx = 2  # Sample 2 ì„ íƒ
advantage = 3.1 - 2.35 = 0.75
weight = exp(0.75 / 2.0) = 1.45

# ëª¨ë¸ì€ Sample 2 (í¬ê²Œ ìš°íšŒ)ë¥¼ í•™ìŠµ
# â†’ ë‹¤ìŒë¶€í„°ëŠ” ì§„í™ í”¼í•¨!
```

---

## ì½”ë“œ ìœ„ì¹˜

í•µì‹¬ ì½”ë“œ: `finetune_consistency.py`

```
Line 105-249:  Visual Safety Scoring
Line 256-323:  Reasoning Analysis
Line 903-1192: _compute_rl_loss (í•µì‹¬!)
  â”œâ”€ 949-988:  Diffusion ìƒ˜í”Œë§
  â”œâ”€ 990-1043: Log prob ê³„ì‚°
  â”œâ”€ 1045-1097: Trust-aware reward ê³„ì‚°
  â””â”€ 1107-1175: AWR loss ê³„ì‚°
```

---

## ê²€ì¦ ì²´í¬ë¦¬ìŠ¤íŠ¸

í•™ìŠµ ì‹œì‘ ì „ í™•ì¸:

- [ ] `rl_loss` ê°’ì´ í•©ë¦¬ì ? (0.1 ~ 10 ë²”ìœ„)
- [ ] `rl_gt_is_best` ì´ˆê¸°ê°’ì´ ë†’ìŒ? (0.8 ~ 1.0)
- [ ] `rl_safety_mean` ê³„ì‚°ë¨? (NaN ì•„ë‹˜)
- [ ] ë©”ëª¨ë¦¬ OOM ì•ˆë‚¨? (ìƒ˜í”Œë§ 4ê°œ â†’ ë©”ëª¨ë¦¬ 4ë°°)

í•™ìŠµ ì¤‘ ëª¨ë‹ˆí„°ë§:

- [ ] `rl_gt_is_best`ê°€ **ê°ì†Œ** ì¶”ì„¸? â† í•µì‹¬!
- [ ] `rl_reward_mean`ì´ **ì¦ê°€** ì¶”ì„¸?
- [ ] `rl_safety_mean`ì´ ì¦ê°€?
- [ ] `rl_loss` ì•ˆì •ì ? (í­ë°œ ì•ˆí•¨)

í•™ìŠµ í›„ ê²€ì¦:

- [ ] `rl_gt_is_best` < 0.5? (ì ˆë°˜ ì´ìƒ ëª¨ë¸ì´ ë‚˜ìŒ)
- [ ] ì¶”ë¡  ì‹œ ì§„í™/ì¥ì• ë¬¼ í”¼í•¨?
- [ ] Reasoning í…ìŠ¤íŠ¸ê°€ ì¼ê´€ì ?

---

## íŠ¸ëŸ¬ë¸”ìŠˆíŒ…

### Q1: `rl_loss`ê°€ NaN ë¨

**ì›ì¸**: AWR weight í­ë°œ (`exp(advantage)`ê°€ ë„ˆë¬´ í¼)

**í•´ê²°**:
```python
weight = torch.exp(advantage / 2.0).clamp(0.1, 5.0)  # Clamp ì¶”ê°€!
```

### Q2: ë©”ëª¨ë¦¬ ë¶€ì¡± (OOM)

**ì›ì¸**: ìƒ˜í”Œë§ 4ê°œ â†’ 4ë°° ë©”ëª¨ë¦¬

**í•´ê²°**:
```bash
--num_trajectory_samples 2  # 4 â†’ 2ë¡œ ì¤„ì´ê¸°
--per_device_train_batch_size 1
--gradient_accumulation_steps 8  # ëŠ˜ë¦¬ê¸°
```

### Q3: `rl_gt_is_best`ê°€ ì•ˆ ì¤„ì–´ë“¦

**ì›ì¸ 1**: GTê°€ ì‹¤ì œë¡œ ì¢‹ìŒ (ë°ì´í„° í’ˆì§ˆ ë†’ìŒ)

**í•´ê²°**: `--safety_reward_weight` ë†’ì´ê¸°

**ì›ì¸ 2**: ìƒ˜í”Œ ë‹¤ì–‘ì„± ë¶€ì¡±

**í•´ê²°**: `--num_trajectory_samples` ëŠ˜ë¦¬ê¸° (4 â†’ 8)

### Q4: í•™ìŠµì´ ë„ˆë¬´ ëŠë¦¼

**ì›ì¸**: Diffusion ìƒ˜í”Œë§ì´ expensive

**í•´ê²°**:
- `--num_trajectory_samples 2`ë¡œ ì¤„ì´ê¸°
- `--rl_loss_weight 0.3`ìœ¼ë¡œ ì¤„ì—¬ì„œ RL ë¹ˆë„ ê°ì†Œ

---

## ë‹¤ìŒ ë‹¨ê³„

1. **í•™ìŠµ ì‹¤í–‰**
   ```bash
   bash run_srd_rl.sh basic
   ```

2. **ë©”íŠ¸ë¦­ ëª¨ë‹ˆí„°ë§**
   - TensorBoardë¡œ `rl_gt_is_best` ì¶”ì 

3. **í‰ê°€**
   - GT í‹€ë¦° ì¼€ì´ìŠ¤ì—ì„œ ëª¨ë¸ ì¶”ë¡ 
   - ì‹œê°í™”: ëª¨ë¸ trajectory vs GT

4. **ë…¼ë¬¸í™”**
   - ì´ ì ‘ê·¼ë²•ì€ novelty ì¶©ë¶„!
   - Learning from Noisy Labels + Safe RL ê²°í•©

---

**êµ¬í˜„ ì™„ë£Œ! ğŸ‰**

ì´ì œ ì‹¤ì œ í•™ìŠµì„ ëŒë ¤ë³´ê³  `rl_gt_is_best`ê°€ ê°ì†Œí•˜ëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”!
